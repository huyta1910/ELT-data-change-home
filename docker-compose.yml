# docker-compose.yml
version: '3.8'

services:
  # 1. Postgres: Airflow's metadata database
  postgres:
    image: postgres:13
    container_name: postgres_airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - data_elt_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow", "-d", "airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. ClickHouse: Your data warehouse
  clickhouse:
    image: clickhouse/clickhouse-server:23.8 # Using a specific stable version
    container_name: clickhouse_server
    ports:
      - "8123:8123" # HTTP interface for clients like the Python script and CloudBeaver
      - "9000:9000" # Native TCP interface
    environment:
      - CLICKHOUSE_DB=analytics_raw
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD} # Comes from the .env file
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    volumes:
      - clickhouse-data:/var/lib/clickhouse/
    networks:
      - data_elt_network
    ulimits: # Important for ClickHouse performance
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144

  # 3. CloudBeaver: The Web UI for ClickHouse
  cloudbeaver:
    image: dbeaver/cloudbeaver:latest
    container_name: cloudbeaver_ui
    ports:
      - "8978:8978" # Access CloudBeaver UI at http://localhost:8978
    volumes:
      - cloudbeaver-data:/opt/cloudbeaver/workspace
    depends_on:
      - clickhouse # Ensures ClickHouse starts before CloudBeaver
    networks:
      - data_elt_network

  # 4. Airflow Services
  # The 'airflow-init' service initializes the database and creates the first user
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_init
    depends_on:
      postgres:
        condition: service_healthy # Waits for Postgres to be ready
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: >
      bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    networks:
      - data_elt_network

  # The main Airflow web UI
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_webserver
    restart: always
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
      - ./dbt_project:/opt/airflow/dbt_project
    networks:
      - data_elt_network
    environment: &airflow_env # YAML anchor for re-using this environment block
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=a_super_secret_key_for_webserver_sessions
      # --- Pass all credentials to the Airflow environment ---
      - SQL_SERVER_HOST=${SQL_SERVER_HOST}
      - SQL_DATABASE=${SQL_DATABASE}
      - SQL_USERNAME=${SQL_USERNAME}
      - SQL_PASSWORD=${SQL_PASSWORD}
      - CLICKHOUSE_HOST=clickhouse # The service name of the ClickHouse container
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      - CLICKHOUSE_DB=analytics_raw
    command: airflow webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  # The Airflow component that runs the scheduled tasks
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - airflow-webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
      - ./dbt_project:/opt/airflow/dbt_project
    networks:
      - data_elt_network
    environment: *airflow_env # Re-uses the environment block from the webserver
    command: airflow scheduler

# Define shared resources
networks:
  data_elt_network:
    driver: bridge

volumes:
  postgres-db-volume:
  clickhouse-data:
  cloudbeaver-data:
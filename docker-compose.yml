# This file defines your entire local development environment
# Run it with: docker-compose up --build -d

services:
  # 1. Postgres: Airflow's metadata database
  postgres:
    image: postgres:13
    container_name: postgres_airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - data_elt_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow", "-d", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 2. Airflow Services
  # This service initializes the Airflow database on the first run
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: >
      bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    networks:
      - data_elt_network

  # The Airflow Webserver (UI)
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_webserver
    restart: always
    depends_on:
      - airflow-init
    ports:
      - "8080:8080" # Access the Airflow UI at http://localhost:8080
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
      - ./dbt_project:/opt/airflow/dbt_project
      # IMPORTANT: Mount your GCP service account key into the container
      - ./gcp-credentials.json:/opt/airflow/gcp/gcp-credentials.json:ro
    networks:
      - data_elt_network
    environment: &airflow_env # YAML anchor to reuse this block
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=a_super_secret_key_for_webserver_sessions
      
      # --- Pass all credentials from the .env file to the Airflow environment ---
      # SQL Server Credentials
      - SQL_SERVER_HOST=${SQL_SERVER_HOST}
      - SQL_DATABASE=${SQL_DATABASE}
      - SQL_USERNAME=${SQL_USERNAME}
      - SQL_PASSWORD=${SQL_PASSWORD}
      
      # Google Cloud Credentials & Config
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - BQ_RAW_DATASET=${BQ_RAW_DATASET}
      # This tells the Google Cloud libraries inside the container where to find the key file
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp/gcp-credentials.json
    command: airflow webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 15s
      timeout: 10s
      retries: 5

  # The Airflow Scheduler (the engine that runs your tasks)
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - airflow-webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
      - ./dbt_project:/opt/airflow/dbt_project
      # The scheduler also needs access to the GCP key
      - ./gcp-credentials.json:/opt/airflow/gcp/gcp-credentials.json:ro
    networks:
      - data_elt_network
    environment: *airflow_env # Re-uses the environment block from the webserver
    command: airflow scheduler

# Define shared resources
networks:
  data_elt_network:
    driver: bridge

volumes:
  postgres-db-volume: